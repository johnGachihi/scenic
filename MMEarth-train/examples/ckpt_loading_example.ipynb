{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "\n",
    "import models.fcmae as fcmae\n",
    "\n",
    "\n",
    "\n",
    "from MODALITIES import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args()\n",
    "\n",
    "# these 4 arguments need to be set manually\n",
    "args.checpoint_dir = 'ckpt/pt-all_mod_atto_1M_128_uncertainty/checkpoint-199.pth' # the directory where the model checkpoints are saved\n",
    "args.random_crop = True # ensure that if the dataset image size is 128 x 128, the resulting image after cropping is 112 x 112.\n",
    "args.random_crop_size = 112 # the size of the crop (either 112 or 56)\n",
    "args.patch_size = 16 # patch size used when pretraining. Either 16 or 8 (for img sizes 112 and 56 respectively)\n",
    "args.loss_aggr = 'uncertainty' # the loss aggregation method. Either 'uncertainty' or 'unweighted'\n",
    "args.use_orig_stem = False # if True the model uses the original stem as in ConvNeXtV2, else it uses the modified MP-MAE stem.\n",
    "args.mask_ratio = 0.6 # the ratio of the mask used in the mask prediction task. Either 0.6 or 0.8\n",
    "\n",
    "\n",
    "# define the input and output bands for the dataset\n",
    "args.inp_modalities = INP_MODALITIES\n",
    "args.out_modalities = OUT_MODALITIES\n",
    "\n",
    "args.modalities = args.inp_modalities.copy()\n",
    "args.modalities.update(args.out_modalities) # args modalities is a dictionary of all the input and output bands.\n",
    "args.modalities_full = MODALITIES_FULL # this is a dictionary of all the bands in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(args.checkpoint_dir, map_location='cpu')\n",
    "model = fcmae.__dict__['convnextv2_atto'](\n",
    "    mask_ratio=args.mask_ratio,\n",
    "    decoder_depth=1,\n",
    "    decoder_embed_dim=512, \n",
    "    norm_pix_loss=True,\n",
    "    patch_size=args.patch_size,\n",
    "    img_size=args.random_crop_size,\n",
    "    args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = model.load_state_dict(checkpoint['model'], strict=False)\n",
    "print(msg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
