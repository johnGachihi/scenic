{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T06:34:58.223833Z",
     "start_time": "2025-02-24T06:34:58.123885Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1240526"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "data_root = Path('/home/admin/john/data/mmearth')\n",
    "\n",
    "splits_path = data_root / \"data_1M_v001_64_splits.json\"\n",
    "indices = json.load(open(splits_path, \"r\"))[\"train\"]\n",
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91bf9d409df97db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "data_path = data_root / 'data_1M_v001_64.h5'\n",
    "data_full = h5py.File(data_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ab8e381-b435-4cd7-806f-a3a39d764ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_info_path = data_root / 'data_1M_v001_64_tile_info.json'\n",
    "with open(tile_info_path, \"r\") as f:\n",
    "    tile_info = json.load(f)\n",
    "\n",
    "band_stats_path = data_root / 'data_1M_v001_64_band_stats.json'\n",
    "with open(band_stats_path, \"r\") as f:\n",
    "    band_stats = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a14e443d-614f-4144-b132-1aefd365c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_stats = band_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb3ef374-983d-4c80-84e0-f323384115f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODALITIES = {\n",
    "    \"sentinel2\": [\n",
    "        \"B1\",\n",
    "        \"B2\",\n",
    "        \"B3\",\n",
    "        \"B4\",\n",
    "        \"B5\",\n",
    "        \"B6\",\n",
    "        \"B7\",\n",
    "        \"B8A\",\n",
    "        \"B8\",\n",
    "        \"B9\",\n",
    "        \"B11\",\n",
    "        \"B12\",\n",
    "    ],\n",
    "    \"sentinel1\": \"all\",\n",
    "}\n",
    "\n",
    "MODALITIES_FULL = {\n",
    "    \"sentinel2\": [\n",
    "        \"B1\",\n",
    "        \"B2\",\n",
    "        \"B3\",\n",
    "        \"B4\",\n",
    "        \"B5\",\n",
    "        \"B6\",\n",
    "        \"B7\",\n",
    "        \"B8A\",\n",
    "        \"B8\",\n",
    "        \"B9\",\n",
    "        \"B10\",\n",
    "        \"B11\",\n",
    "        \"B12\",\n",
    "    ],\n",
    "    \"sentinel2_cloudmask\": [\"QA60\"],\n",
    "    \"sentinel2_cloudprod\": [\"MSK_CLDPRB\"],\n",
    "    \"sentinel2_scl\": [\"SCL\"],\n",
    "    \"sentinel1\": [\n",
    "        \"asc_VV\",\n",
    "        \"asc_VH\",\n",
    "        \"asc_HH\",\n",
    "        \"asc_HV\",\n",
    "        \"desc_VV\",\n",
    "        \"desc_VH\",\n",
    "        \"desc_HH\",\n",
    "        \"desc_HV\",\n",
    "    ],\n",
    "    \"aster\": [\"elevation\", \"slope\"],\n",
    "    \"era5\": [\n",
    "        \"prev_month_avg_temp\",\n",
    "        \"prev_month_min_temp\",\n",
    "        \"prev_month_max_temp\",\n",
    "        \"prev_month_total_precip\",\n",
    "        \"curr_month_avg_temp\",\n",
    "        \"curr_month_min_temp\",\n",
    "        \"curr_month_max_temp\",\n",
    "        \"curr_month_total_precip\",\n",
    "        \"year_avg_temp\",\n",
    "        \"year_min_temp\",\n",
    "        \"year_max_temp\",\n",
    "        \"year_total_precip\",\n",
    "    ],\n",
    "    \"dynamic_world\": [\"landcover\"],\n",
    "    \"canopy_height_eth\": [\"height\", \"std\"],\n",
    "    \"lat\": [\"sin\", \"cos\"],\n",
    "    \"lon\": [\"sin\", \"cos\"],\n",
    "    \"biome\": [\"biome\"],\n",
    "    \"eco_region\": [\"eco_region\"],\n",
    "    \"month\": [\"sin_month\", \"cos_month\"],\n",
    "    \"esa_worldcover\": [\"map\"],\n",
    "}\n",
    "\n",
    "NO_DATA_VAL = {\n",
    "    \"sentinel2\": 0,\n",
    "    \"sentinel2_cloudmask\": 65535,\n",
    "    \"sentinel2_cloudprod\": 65535,\n",
    "    \"sentinel2_scl\": 255,\n",
    "    \"sentinel1\": float(\"-inf\"),\n",
    "    \"aster\": float(\"-inf\"),\n",
    "    \"canopy_height_eth\": 255,\n",
    "    \"dynamic_world\": 0,\n",
    "    \"esa_worldcover\": 255,\n",
    "    \"lat\": float(\"-inf\"),\n",
    "    \"lon\": float(\"-inf\"),\n",
    "    \"month\": float(\"-inf\"),\n",
    "    \"era5\": float(\"inf\"),\n",
    "    \"biome\": 255,\n",
    "    \"eco_region\": 65535,\n",
    "}\n",
    "\n",
    "modalities_full = MODALITIES_FULL\n",
    "modalities = MODALITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8abfa01e-6273-4971-8d48-9e336bfc9e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modality sentinel2\n",
      "modality_idx [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12]\n",
      "data (12, 64, 64) [1627 1625 1629 1640 1650]\n",
      "data (12, 64, 64) [-0.15649307 -0.1578088  -0.15517734 -0.14794082 -0.14136217]\n",
      "\n",
      "modality sentinel1\n",
      "modality_idx [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "data (8, 64, 64) [-7.246237  -8.437068  -9.1128235 -9.794962  -9.71821  ]\n",
      "data (8, 64, 64) [0.86320915 0.6310232  0.49926571 0.36626369 0.38122859]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12, 64, 64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "idx = 0\n",
    "return_dict = OrderedDict()\n",
    "name = data_full['metadata'][indices[idx]][0].decode(\"utf-8\")\n",
    "l2a = tile_info[name][\"S2_type\"] == \"l2a\"\n",
    "\n",
    "for modality in modalities.keys():\n",
    "    print(\"modality\", modality)\n",
    "    if modalities[modality] == \"all\":\n",
    "        modality_idx = [i for i in range(len(modalities_full[modality]))]\n",
    "    else:\n",
    "        modality_idx = [\n",
    "            modalities_full[modality].index(m)\n",
    "            for m in modalities[modality]\n",
    "        ]\n",
    "    print('modality_idx', modality_idx)\n",
    "\n",
    "    data = data_full[modality][indices[idx], modality_idx, ...]\n",
    "    data = np.array(data)\n",
    "    print('data', data.shape, data[0, 0, :5])\n",
    "\n",
    "    if modality == \"sentinel2\":\n",
    "        modality_ = \"sentinel2_l2a\" if l2a else \"sentinel2_l1c\"\n",
    "    else:\n",
    "        modality_ = modality\n",
    "\n",
    "    if modality not in [\"biome\", \"eco_region\", \"dynamic_world\", \"esa_worldcover\"]:\n",
    "        means = np.array(norm_stats[modality_][\"mean\"])[modality_idx]\n",
    "        stds = np.array(norm_stats[modality_][\"std\"])[modality_idx]\n",
    "        if modality in [\"era5\", \"lat\", \"lon\", \"month\"]:\n",
    "            # single value mean and std\n",
    "            data = (data - means) / stds\n",
    "        else:\n",
    "            # single value mean and std for each band\n",
    "            data = (data - means[:, None, None]) / stds[:, None, None]\n",
    "    print('data', data.shape, data[0, 0, :5])\n",
    "\n",
    "    data = (\n",
    "        np.where(data == NO_DATA_VAL[modality], np.nan, data)\n",
    "        if modality != \"dynamic_world\"\n",
    "        else data\n",
    "    )\n",
    "\n",
    "    data = data.astype(np.dtype(\"float32\"))\n",
    "\n",
    "    return_dict[modality] = data\n",
    "\n",
    "    print()\n",
    "\n",
    "return_dict['sentinel2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e41db-42b4-498b-9623-026812660d62",
   "metadata": {},
   "source": [
    "# TFDS Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e02a3aa-1de0-4b15-ad2c-65d3cc648f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODALITIES = {\n",
    "    \"sentinel2\": [\n",
    "        \"B1\",\n",
    "        \"B2\",\n",
    "        \"B3\",\n",
    "        \"B4\",\n",
    "        \"B5\",\n",
    "        \"B6\",\n",
    "        \"B7\",\n",
    "        \"B8A\",\n",
    "        \"B8\",\n",
    "        \"B9\",\n",
    "        \"B11\",\n",
    "        \"B12\",\n",
    "    ],\n",
    "    \"sentinel1\": \"all\",\n",
    "}\n",
    "\n",
    "MODALITIES_FULL = {\n",
    "    \"sentinel2\": [\n",
    "        \"B1\",\n",
    "        \"B2\",\n",
    "        \"B3\",\n",
    "        \"B4\",\n",
    "        \"B5\",\n",
    "        \"B6\",\n",
    "        \"B7\",\n",
    "        \"B8A\",\n",
    "        \"B8\",\n",
    "        \"B9\",\n",
    "        \"B10\",\n",
    "        \"B11\",\n",
    "        \"B12\",\n",
    "    ],\n",
    "    \"sentinel2_cloudmask\": [\"QA60\"],\n",
    "    \"sentinel2_cloudprod\": [\"MSK_CLDPRB\"],\n",
    "    \"sentinel2_scl\": [\"SCL\"],\n",
    "    \"sentinel1\": [\n",
    "        \"asc_VV\",\n",
    "        \"asc_VH\",\n",
    "        \"asc_HH\",\n",
    "        \"asc_HV\",\n",
    "        \"desc_VV\",\n",
    "        \"desc_VH\",\n",
    "        \"desc_HH\",\n",
    "        \"desc_HV\",\n",
    "    ],\n",
    "    \"aster\": [\"elevation\", \"slope\"],\n",
    "    \"era5\": [\n",
    "        \"prev_month_avg_temp\",\n",
    "        \"prev_month_min_temp\",\n",
    "        \"prev_month_max_temp\",\n",
    "        \"prev_month_total_precip\",\n",
    "        \"curr_month_avg_temp\",\n",
    "        \"curr_month_min_temp\",\n",
    "        \"curr_month_max_temp\",\n",
    "        \"curr_month_total_precip\",\n",
    "        \"year_avg_temp\",\n",
    "        \"year_min_temp\",\n",
    "        \"year_max_temp\",\n",
    "        \"year_total_precip\",\n",
    "    ],\n",
    "    \"dynamic_world\": [\"landcover\"],\n",
    "    \"canopy_height_eth\": [\"height\", \"std\"],\n",
    "    \"lat\": [\"sin\", \"cos\"],\n",
    "    \"lon\": [\"sin\", \"cos\"],\n",
    "    \"biome\": [\"biome\"],\n",
    "    \"eco_region\": [\"eco_region\"],\n",
    "    \"month\": [\"sin_month\", \"cos_month\"],\n",
    "    \"esa_worldcover\": [\"map\"],\n",
    "}\n",
    "\n",
    "NO_DATA_VAL = {\n",
    "    \"sentinel2\": 0,\n",
    "    \"sentinel2_cloudmask\": 65535,\n",
    "    \"sentinel2_cloudprod\": 65535,\n",
    "    \"sentinel2_scl\": 255,\n",
    "    \"sentinel1\": float(\"-inf\"),\n",
    "    \"aster\": float(\"-inf\"),\n",
    "    \"canopy_height_eth\": 255,\n",
    "    \"dynamic_world\": 0,\n",
    "    \"esa_worldcover\": 255,\n",
    "    \"lat\": float(\"-inf\"),\n",
    "    \"lon\": float(\"-inf\"),\n",
    "    \"month\": float(\"-inf\"),\n",
    "    \"era5\": float(\"inf\"),\n",
    "    \"biome\": 255,\n",
    "    \"eco_region\": 65535,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb5c978c-e551-493d-8f52-2dfb7da7bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "class MMEarthBuilder(tfds.core.GeneratorBasedBuilder):\n",
    "    VERSION = tfds.core.Version('0.0.1')\n",
    "    \n",
    "    def __init__(self, modalities: dict, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.modalities = modalities\n",
    "\n",
    "    def _info(self):\n",
    "        return tfds.core.DatasetInfo(\n",
    "            builder=self,\n",
    "            features=tfds.features.FeaturesDict({\n",
    "                'sentinel2': tfds.features.Tensor(shape=(12, 64, 64), dtype=np.dtype(\"float32\")),\n",
    "                'sentinel1': tfds.features.Tensor(shape=(8, 64, 64), dtype=np.dtype(\"float32\")),\n",
    "                'id': tfds.features.Text(),\n",
    "            }),\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        data_root = Path('/home/admin/john/data/mmearth')\n",
    "\n",
    "        # Full data\n",
    "        data_path = data_root / 'data_1M_v001_64.h5'\n",
    "        data_full = h5py.File(data_path, 'r')\n",
    "\n",
    "        # Split indices\n",
    "        splits_path = data_root / 'data_1M_v001_64_splits.json'\n",
    "        with open(splits_path, \"r\") as f:\n",
    "            indices = json.load(f)[\"train\"][:10000]\n",
    "\n",
    "        # Tile info\n",
    "        tile_info_path = data_root / 'data_1M_v001_64_tile_info.json'\n",
    "        with open(tile_info_path, \"r\") as f:\n",
    "            tile_info = json.load(f)\n",
    "\n",
    "        # Band norm stats\n",
    "        band_stats_path = data_root / 'data_1M_v001_64_band_stats.json'\n",
    "        with open(band_stats_path, \"r\") as f:\n",
    "            norm_stats = json.load(f)\n",
    "        \n",
    "        return {\n",
    "            'train': self._generate_examples(data_full, indices, tile_info, norm_stats)\n",
    "        }\n",
    "\n",
    "    def _generate_examples(self, data_full, indices, tile_info, norm_stats):\n",
    "        for idx in indices:\n",
    "            return_dict = OrderedDict()\n",
    "            name = data_full['metadata'][idx][0].decode(\"utf-8\")\n",
    "            l2a = tile_info[name][\"S2_type\"] == \"l2a\"\n",
    "\n",
    "            for modality in self.modalities.keys():\n",
    "                # Get band indices\n",
    "                if self.modalities[modality] == \"all\":\n",
    "                    modality_idx = [i for i in range(len(MODALITIES_FULL[modality]))]\n",
    "                else:\n",
    "                    modality_idx = [MODALITIES_FULL[modality].index(m) for m in self.modalities[modality]]\n",
    "\n",
    "                # Get data\n",
    "                data = data_full[modality][idx, modality_idx, ...]\n",
    "                data = np.array(data)\n",
    "\n",
    "                # inside the band_stats, the name for sentinel2 is sentinel2_l1c or sentinel2_l2a\n",
    "                if modality == \"sentinel2\":\n",
    "                    modality_ = \"sentinel2_l2a\" if l2a else \"sentinel2_l1c\"\n",
    "                else:\n",
    "                    modality_ = modality\n",
    "\n",
    "                means = np.array(norm_stats[modality_][\"mean\"])[modality_idx]\n",
    "                stds = np.array(norm_stats[modality_][\"std\"])[modality_idx]\n",
    "                data = (data - means[:, None, None]) / stds[:, None, None]  # Why the `None`s\n",
    "\n",
    "                # converting the nodata values to nan to keep everything consistent\n",
    "                data = (\n",
    "                    np.where(data == NO_DATA_VAL[modality], np.nan, data)\n",
    "                    if modality != \"dynamic_world\"\n",
    "                    else data\n",
    "                )\n",
    "\n",
    "                data = data.astype(np.dtype(\"float32\"))\n",
    "\n",
    "                return_dict[modality] = data\n",
    "\n",
    "            return_dict[\"id\"] = name\n",
    "\n",
    "            yield name, return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80b6b643-85e9-4236-8ccb-7c1fc38397a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 08:37:03.688164: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-24 08:37:03.688218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-24 08:37:03.688241: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-24 08:37:03.695086: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-24 08:37:05.357495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-24 08:37:05.362317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-24 08:37:05.362451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-24 08:37:05.363793: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-24 08:37:05.363908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-24 08:37:05.363996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-24 08:37:05.575652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-24 08:37:05.575769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-24 08:37:05.575852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-24 08:37:05.575930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14842 MB memory:  -> device: 0, name: NVIDIA L40-48Q, pci bus id: 0000:02:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': <_PrefetchDataset element_spec={'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'sentinel1': TensorSpec(shape=(8, 64, 64), dtype=tf.float32, name=None), 'sentinel2': TensorSpec(shape=(12, 64, 64), dtype=tf.float32, name=None)}>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = MMEarthBuilder(modalities=MODALITIES)\n",
    "builder.download_and_prepare(\n",
    "    download_dir='/home/admin/john/data/mmearth_',\n",
    "    download_config=tfds.download.DownloadConfig(manual_dir='/home/admin/john/data/mmearth')\n",
    ")\n",
    "builder.as_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7c060d-bb7c-4531-b38c-6537c30cfd9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <_PrefetchDataset element_spec={'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'sentinel1': TensorSpec(shape=(8, 64, 64), dtype=tf.float32, name=None), 'sentinel2': TensorSpec(shape=(12, 64, 64), dtype=tf.float32, name=None)}>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mmearth.mmearth_dataset import MMEarthBuilder\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "MODALITIES = {\n",
    "    \"sentinel2\": [\n",
    "        \"B1\",\n",
    "        \"B2\",\n",
    "        \"B3\",\n",
    "        \"B4\",\n",
    "        \"B5\",\n",
    "        \"B6\",\n",
    "        \"B7\",\n",
    "        \"B8A\",\n",
    "        \"B8\",\n",
    "        \"B9\",\n",
    "        \"B11\",\n",
    "        \"B12\",\n",
    "    ],\n",
    "    \"sentinel1\": \"all\",\n",
    "}\n",
    "\n",
    "builder = MMEarthBuilder(modalities=MODALITIES)\n",
    "\n",
    "builder.as_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21d8fcb7-8513-4649-963f-05cbcd38fd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/admin/tensorflow_datasets/imagenet/5.1.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:ImageNet 2012 Challenge test split not found at /home/admin/john/data/ImageNet/ILSVRC2012_img_test.tar. Proceeding with data generation anyways but the split will be missing from the dataset...\n",
      "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]\n",
      "\u001b[Aerating validation examples...: 0 examples [00:00, ? examples/s]\n",
      "\u001b[Aerating validation examples...: 4514 examples [00:01, 4513.37 examples/s]\n",
      "\u001b[Aerating validation examples...: 9028 examples [00:02, 3106.51 examples/s]\n",
      "\u001b[Aerating validation examples...: 12387 examples [00:03, 3040.16 examples/s]\n",
      "\u001b[Aerating validation examples...: 15555 examples [00:04, 3034.70 examples/s]\n",
      "\u001b[Aerating validation examples...: 18666 examples [00:06, 2782.89 examples/s]\n",
      "\u001b[Aerating validation examples...: 21511 examples [00:07, 2753.88 examples/s]\n",
      "\u001b[Aerating validation examples...: 24303 examples [00:08, 2609.02 examples/s]\n",
      "\u001b[Aerating validation examples...: 26939 examples [00:09, 2555.05 examples/s]\n",
      "\u001b[Aerating validation examples...: 29509 examples [00:11, 1932.23 examples/s]\n",
      "\u001b[Aerating validation examples...: 31966 examples [00:12, 2054.51 examples/s]\n",
      "\u001b[Aerating validation examples...: 34820 examples [00:13, 2254.38 examples/s]\n",
      "\u001b[Aerating validation examples...: 37247 examples [00:14, 2294.81 examples/s]\n",
      "\u001b[Aerating validation examples...: 40059 examples [00:15, 2435.75 examples/s]\n",
      "\u001b[Aerating validation examples...: 42921 examples [00:16, 2555.06 examples/s]\n",
      "\u001b[Aerating validation examples...: 45620 examples [00:17, 2596.02 examples/s]\n",
      "\u001b[Aerating validation examples...: 48599 examples [00:18, 2706.99 examples/s]\n",
      "\u001b[A                                                                          \n",
      "\u001b[Affling /home/admin/tensorflow_datasets/imagenet/incomplete.E6I1V3_5.1.0/imagenet-validation.tfrecord*...:   0%|          | 0/50000 [00:00<?, ? examples/s]\n",
      "\u001b[Affling /home/admin/tensorflow_datasets/imagenet/incomplete.E6I1V3_5.1.0/imagenet-validation.tfrecord*...:  12%|█▏        | 6102/50000 [00:01<00:07, 6101.41 examples/s]\n",
      "\u001b[Affling /home/admin/tensorflow_datasets/imagenet/incomplete.E6I1V3_5.1.0/imagenet-validation.tfrecord*...:  24%|██▍       | 12206/50000 [00:02<00:06, 6102.60 examples/s]\n",
      "\u001b[Affling /home/admin/tensorflow_datasets/imagenet/incomplete.E6I1V3_5.1.0/imagenet-validation.tfrecord*...:  37%|███▋      | 18311/50000 [00:03<00:05, 6091.38 examples/s]\n",
      "\u001b[Affling /home/admin/tensorflow_datasets/imagenet/incomplete.E6I1V3_5.1.0/imagenet-validation.tfrecord*...:  49%|████▉     | 24541/50000 [00:04<00:04, 6145.96 examples/s]\n",
      "\u001b[Affling /home/admin/tensorflow_datasets/imagenet/incomplete.E6I1V3_5.1.0/imagenet-validation.tfrecord*...:  61%|██████▏   | 30688/50000 [00:05<00:03, 6058.12 examples/s]\n",
      "\u001b[Affling /home/admin/tensorflow_datasets/imagenet/incomplete.E6I1V3_5.1.0/imagenet-validation.tfrecord*...:  73%|███████▎  | 36749/50000 [00:06<00:02, 5946.98 examples/s]\n",
      "\u001b[Affling /home/admin/tensorflow_datasets/imagenet/incomplete.E6I1V3_5.1.0/imagenet-validation.tfrecord*...:  85%|████████▌ | 42701/50000 [00:07<00:01, 5909.68 examples/s]\n",
      "\u001b[Affling /home/admin/tensorflow_datasets/imagenet/incomplete.E6I1V3_5.1.0/imagenet-validation.tfrecord*...:  98%|█████████▊| 48856/50000 [00:08<00:00, 5980.87 examples/s]\n",
      "                                                                                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imagenet downloaded and prepared to /home/admin/tensorflow_datasets/imagenet/5.1.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from imagenet import Imagenet\n",
    "\n",
    "builder = Imagenet()\n",
    "builder.download_and_prepare(\n",
    "    download_dir='/home/admin/john/data/imagenet2012',\n",
    "    download_config=tfds.download.DownloadConfig(\n",
    "        manual_dir='/home/admin/john/data/ImageNet')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
